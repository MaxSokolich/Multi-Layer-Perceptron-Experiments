{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the MLP model\n",
    "# Define the MLP model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=128, num_discrete_actions=20):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, int(2*hidden_size))\n",
    "        self.fc3 = nn.Linear(int(2*hidden_size), hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # Output layers\n",
    "        self.fc_action = nn.Linear(hidden_size, num_discrete_actions)  # Discrete action classification\n",
    "        self.fc_heading = nn.Linear(hidden_size, 1)  # Continuous heading direction\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu(x)\n",
    "        action_logits = self.fc_action(x)  # No activation (will apply softmax in loss function)\n",
    "        heading = self.fc_heading(x)  # No activation (direct regression for heading)\n",
    "        return action_logits, heading\n",
    "    \n",
    "\n",
    "\n",
    "class MLP_continuous(nn.Module):\n",
    "    def __init__(self, input_size=4, output_size=2):\n",
    "        super(MLP_continuous, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(64, 256)\n",
    "        self.fc3 = nn.Linear(256, 256)\n",
    "         \n",
    "        # Output layers\n",
    "        self.fc_action = nn.Linear(256, 20)  # Discrete action classification\n",
    "        self.fc_heading = nn.Linear(256, 1)  # Continuous heading direction\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu(x)\n",
    "        action = self.relu(self.fc_action(x))\n",
    "        heading = self.fc_heading(x)\n",
    "        return action, heading\n",
    "\n",
    "\n",
    "\n",
    "def load_dataset(state_data, action_data, heading_data, batch_size=64, train_split=0.8):\n",
    "    states = torch.tensor(state_data, dtype=torch.float32)\n",
    "    actions = torch.tensor(action_data, dtype=torch.long)  # Discrete actions as integer labels\n",
    "    headings = torch.tensor(heading_data, dtype=torch.float32)\n",
    "    dataset = TensorDataset(states, actions, headings)\n",
    "    \n",
    "    train_size = int(train_split * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    return train_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_data = np.load(\"all_pos.npy\")\n",
    "heading_data = np.load('all_alpha.npy')\n",
    "action_data = np.load('all_frq.npy')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader = load_dataset(state_data, action_data, heading_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Train Loss: 8979.1897, Test Loss: 4.6933\n",
      "Epoch [2/50], Train Loss: 7747.7999, Test Loss: 4.4855\n",
      "Epoch [3/50], Train Loss: 7483.0420, Test Loss: 4.3625\n",
      "Epoch [4/50], Train Loss: 7336.5669, Test Loss: 4.2472\n",
      "Epoch [5/50], Train Loss: 7236.6977, Test Loss: 4.2101\n",
      "Epoch [6/50], Train Loss: 7158.6290, Test Loss: 4.2568\n",
      "Epoch [7/50], Train Loss: 7098.8899, Test Loss: 4.1529\n",
      "Epoch [8/50], Train Loss: 7035.1731, Test Loss: 4.1683\n",
      "Epoch [9/50], Train Loss: 6988.4838, Test Loss: 4.1140\n",
      "Epoch [10/50], Train Loss: 6941.0660, Test Loss: 4.0986\n",
      "Epoch [11/50], Train Loss: 6897.2750, Test Loss: 4.0827\n",
      "Epoch [12/50], Train Loss: 6859.1619, Test Loss: 4.0755\n",
      "Epoch [13/50], Train Loss: 6821.4652, Test Loss: 4.0985\n",
      "Epoch [14/50], Train Loss: 6787.6444, Test Loss: 4.0711\n",
      "Epoch [15/50], Train Loss: 6762.5361, Test Loss: 4.0834\n",
      "Epoch [16/50], Train Loss: 6728.8118, Test Loss: 4.0144\n",
      "Epoch [17/50], Train Loss: 6701.0492, Test Loss: 4.0217\n",
      "Epoch [18/50], Train Loss: 6667.7885, Test Loss: 4.0181\n",
      "Epoch [19/50], Train Loss: 6649.6277, Test Loss: 3.9772\n",
      "Epoch [20/50], Train Loss: 6625.6218, Test Loss: 3.9992\n",
      "Epoch [21/50], Train Loss: 6595.9207, Test Loss: 4.0577\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[79], line 71\u001b[0m\n\u001b[1;32m     68\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# Run training\u001b[39;00m\n\u001b[0;32m---> 71\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion_heading\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# Run training\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[79], line 33\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, test_loader, criterion_action, criterion_heading, optimizer, num_epochs)\u001b[0m\n\u001b[1;32m     30\u001b[0m loss_heading \u001b[38;5;241m=\u001b[39m criterion_heading(weights\u001b[38;5;241m*\u001b[39mheading_pred\u001b[38;5;241m.\u001b[39msqueeze(), weights\u001b[38;5;241m*\u001b[39mheadings)\n\u001b[1;32m     31\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_action \u001b[38;5;241m+\u001b[39m  loss_heading\n\u001b[0;32m---> 33\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     36\u001b[0m total_train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/anaconda3/envs/stable_RL/lib/python3.10/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/stable_RL/lib/python3.10/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/stable_RL/lib/python3.10/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = MLP(input_size=4, num_discrete_actions=20).to(device)\n",
    "model = MLP_continuous(input_size=4, output_size=2).to(device)\n",
    "criterion_action = nn.CrossEntropyLoss()\n",
    "# criterion_action = nn.MSELoss()\n",
    "criterion_heading = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Training loop\n",
    "def train_model(model, train_loader, test_loader, criterion_action, criterion_heading, optimizer, num_epochs=5):\n",
    "    model.train()\n",
    "    train_loss_history = []\n",
    "    test_loss_history = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_train_loss = 0\n",
    "        weights = 60\n",
    "        for states, actions, headings in train_loader:\n",
    "            states = states.to(device)\n",
    "            actions = actions.to(device).float()\n",
    "            headings = headings.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            action_logits, heading_pred = model(states)\n",
    "            \n",
    "            loss_action = criterion_action(action_logits.squeeze(), actions.long())\n",
    "            loss_heading = criterion_heading(weights*heading_pred.squeeze(), weights*headings)\n",
    "            loss = loss_action +  loss_heading\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_train_loss += loss.item()\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        train_loss_history.append(avg_train_loss)\n",
    "        \n",
    "        # Evaluate on test data\n",
    "        model.eval()\n",
    "        total_test_loss = 0\n",
    "        avg_test_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for states, actions, headings in test_loader:\n",
    "                states = states.to(device)\n",
    "                actions = actions.to(device)\n",
    "                headings = headings.to(device)\n",
    "                action_logits, heading_pred = model(states)\n",
    "                \n",
    "                loss_action = criterion_action(action_logits, actions.long())\n",
    "                loss_heading = criterion_heading(heading_pred.squeeze(), headings)\n",
    "                loss = loss_action + loss_heading\n",
    "                total_test_loss += loss.item()\n",
    "        avg_test_loss = total_test_loss / len(test_loader)\n",
    "        test_loss_history.append(avg_test_loss)\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Test Loss: {avg_test_loss:.4f}\")\n",
    "        \n",
    "    # Plot loss function\n",
    "    plt.plot(range(1, num_epochs + 1), train_loss_history, marker='o', label='Train Loss')\n",
    "    plt.plot(range(1, num_epochs + 1), test_loss_history, marker='s', label='Test Loss')\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training and Test Loss Over Epochs\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "# Run training\n",
    "train_model(model, train_loader, test_loader, criterion_action, criterion_heading, optimizer, num_epochs=50)\n",
    "\n",
    "# Run training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# torch.save(model.state_dict(), \"mlp_model.pth\")\n",
    "torch.save(model.state_dict(), \"mlp_model_continuous.pth\")\n",
    "print(\"Model saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for states, actions in test_loader:\n",
    "            states = states.to(device)\n",
    "            actions = actions.to(device)\n",
    "            outputs = model(states)\n",
    "            loss = criterion(outputs, actions)\n",
    "            total_loss += loss.item()\n",
    "    print(f\"Test Loss: {total_loss/len(test_loader):.4f}\")\n",
    "\n",
    "def infer_action(model, state):\n",
    "    model.eval()\n",
    "    state = torch.tensor(state, dtype=torch.float32).to(device).unsqueeze(0)  # Add batch dimension\n",
    "    with torch.no_grad():\n",
    "        action_logits, heading = model(state)\n",
    "        action_probabilities = torch.softmax(action_logits, dim=1)\n",
    "        action = torch.argmax(action_probabilities, dim=1).item()  # Get the discrete action\n",
    "        heading = heading.item()  # Get the continuous heading value\n",
    "    return action, heading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx: 182776\n",
      "Predicted Action: 8 True Action: 8\n",
      "Predicted Heading: 1.591921329498291 True Heading: -0.39269909262657166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3977939/834229402.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  state = torch.tensor(state, dtype=torch.float32).to(device).unsqueeze(0)  # Add batch dimension\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(len(test_loader.dataset))\n",
    "print('idx:', idx)\n",
    "sample_state = test_loader.dataset[idx][0].to(device)\n",
    "sample_action = test_loader.dataset[idx][1].to(device)\n",
    "sample_heading = test_loader.dataset[idx][2].to(device)\n",
    "action_out, heading_out = infer_action(model, sample_state)\n",
    "print(\"Predicted Action:\", action_out, \"True Action:\", sample_action.item())\n",
    "print(\"Predicted Heading:\", heading_out, \"True Heading:\", sample_heading.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx: 714255\n",
      "Predicted Action: 4 True Action: 2\n",
      "Predicted Heading: 63.3470129290118 True Heading: 180.00000500895632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3977939/834229402.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  state = torch.tensor(state, dtype=torch.float32).to(device).unsqueeze(0)  # Add batch dimension\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(len(train_loader.dataset))\n",
    "print('idx:', idx)\n",
    "sample_state = train_loader.dataset[idx][0].to(device)\n",
    "sample_action = train_loader.dataset[idx][1].to(device)\n",
    "sample_heading = train_loader.dataset[idx][2].to(device)\n",
    "action_out, heading_out = infer_action(model, sample_state)\n",
    "print(\"Predicted Action:\", action_out, \"True Action:\", sample_action.item())\n",
    "print(\"Predicted Heading:\", heading_out*180/np.pi, \"True Heading:\", sample_heading.item()/np.pi*180)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stable_RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
